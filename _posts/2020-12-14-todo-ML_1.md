---
title: "ToDo-ML_1_Ensemble_1/2"
date: 2020-12-14
categories: Machine Learning 
---

### ToDo-ML_1_Ensemble_1/2; 학습모델 오류 이해
Ensemble(앙상블)기법이란?_1/2

머신러닝 기법 중 하나인<br> 
**Ensemble(앙상블)기법**이란<br>
샘플링된 각 데이터 셋을 여러 모델들을 통해 학습시킨 후 

각 모델의 예측 결과를 집계하여<br> 
더욱 정확한 예측 결과를 뽑아내는 기법을 말합니다.

Ensemble기법을 이해하기 위해선,<br>
학습 모델의 예측 오류를 이해하여야 합니다.

Ensemble기법의 목적이 **그 오류를 해결**하는 것이기 때문입니다.


### ■ 학습 모델의 예측 오류


학습 모델의 예측 오류는 크게
- 1. Bias(편향)

- 2. Variance(분산)

으로 나뉘고 이는 각각<br> 
**under-fitting** 과 **over-fitting** 로 이어집니다.

### 1. Bias

예측값들과 실제 정답의 차이를 측정합니다.<br>
주로 **"제한된 특성"** 을 가진 예측을 수행할 때 Bias는 높아집니다.

차이: (E[f^(x)]-f(x))^2

수식: f(x)=정답, f^(x)=예측값, E[ ]=기대값=평균 
-> 즉, 예측값들의 평균과 실제 정답의 차이를 제곱

(예: 대한민국 유권자의 투표 결과 예측을 "지역이라는 제한된 변수"로만 수행)

높은 Bias를 가진 학습 모델은,<br>
학습 데이터를 충분히 표현하지 못하므로<br>
예측값들이 낮은 정답률을 보이게 됩니다.

이러한 모델을 **Under-fitting** 되었다고 합니다.<br>
(=모델의 Complexity 복잡성이 감소함)


### 2. Variance

각 모델들의 예측값들 간의 차이를 측정합니다.<br>
E[f^(x)-E[f^(x)]]^2

수식: f(x)=정답, f^(x)=예측값, E[ ]=기대값=평균<br> 
-> 즉, 각 예측값과 예측값들의 평균간의 차이를 제곱


주로 **"제한된 수"** 의 표본에서<br> 
예측을 수행할 때 Variance는 높아집니다.
(예: 대한민국 유권자의 투표 결과 예측을 "서울이라는 적은 수"의 표본에서만 수행)

높은 Variance를 가진 학습 모델은,<br>
학습 데이터셋에 너무 민감하여(=그 학습 데이터셋에만 최적화)

해당 학습 데이터셋에서는 잘 예측할 수 있어도<br>
새로운 데이터 셋에는 잘 작동하지 못하게 됩니다.

이러한 모델을 **Over-fitting** 되었다고 합니다.<br>
(=모델의 복잡성이 증가함)


### ■ Bias와 Variance의 trade-off

그렇다면,낮은 Bias와 낮은 Variance를 가지는 모델의<br>
예측값들이 높은 정답률을 보일 것임을 알 수 있습니다. 

 
하지만 보통의 경우, 두 오류는<br>
아래와 같은 **Trade-off** 관계를 가지고 있습니다. 

- Variance는 낮은데 Bias는 높음<br>
 (=모델이 간단함=under-fitting)

- Variance는 높지만 Bias는 낮음<br> 
 (=모델이 복잡함=over-fitting)

이는 모델이 데이터를<br> 
반복 학습하는 수(K)를 조정할 때 주로 발생합니다. 


예로, 데이터를 반복 학습시키는 수가 많아지면,<br> 
(=모델이 점점 복잡해지면)

Bias는 감소하고,<br> 
(= 학습 데이터셋을 반복학습 하다보니 <br>
예측값들이 정답을 잘 예측하게 됩니다.)

Variance는 점점 증가합니다.<br>
(= 학습 데이터셋만을 외우다시피 반복하니, <br>
다른 데이터셋은 잘 예측하지 못하게 됩니다.)

따라서 모델의 복잡성의<br> 
**최적화(Optimum Model Complexity)** 작업이 필요합니다. 


이는 즉, Total Error의 최소점을 찾는 것을 의미하고<br>
이 때 **Ensemble기법** 이 적용되게 됩니다. 

Total Error= Bias+Variance+근본적 오차irreducible error<br>

수식: f(x)=정답, f^(x)=예측값, E[ ]=기대값=평균, σ=근본적 오차<br>
Error(x)=(E[f^(x)]-f(x))^2 + E[f^(x)-E[f^(x)]]^2 + σe^2



### ■ Ensemble기법 유형

Ensemble기법은 기본적으로 2가지로 나뉩니다.


- 1. Bagging(배깅)

- 2. Boosting(부스팅)


두 기법 모두 오류(예측값들의 손실)를 최소화하기 위해<br>
약한 분류기(예측값이 정답과 다른 데이터 셋)들을 결합하여<br>

하나의 강한 분류기(실제값과 거의 같은 데이터 셋)을 생성하는 방법이지만,<br>
그 **"결합의 방법"** 에서 차이가 나타납니다.

자세한 내용은 [다음 포스팅](https://tododata101.github.io/machine/learning/dataanalysis/todo-ML_2/)에서 이어가도록 하겠습니다.

*위 내용은 본인의 티스토리 포스팅과 일치합니다.*

[Tistory Posting](https://todo-data.tistory.com/5)
