---
title: "ToDo-ML_1_Ensemble_2/2"
date: 2020-12-15
categories: Machine Learning  
---

### ToDo-ML_1_Ensemble_2/2; 앙상블 기법 유형 

Ensemble(앙상블)기법이란?_2/2

### ■ Ensemble기법 유형

1. Bagging(배깅)

2. Boosting(부스팅)

-------------

#### 1. Bagging


Bagging은 Bootstrap Aggregating의 약어로,<br>

통계학에서의 Bootstrap은<br> 
한 표본을 여러 데이터셋으로 다시 샘플링(Resampling)하는 것을 의미하고

Aggregating은 집계를 의미하여,<br>

쉽게 말해, 한 표본을 resampling하여 집계하는 기법으로.
Bagging은 특히 High Variance의 경우 적합합니다.<br>

##### Resampling
Resampling된 데이터셋들에 적용된<br> 
각각의 모델들의 예측값은 서로 다르고,<br>
그 다름(High Variance)을 해결하기 위해,<br>
각기 다른 예측값들을 가지는 데이터 셋(약한 분류기=weak learner)*들을 집계하여

하나의 데이터 셋(강한 분류기=stong learner)를 생성하여 Variance를 낮춥니다.


*참고: 어떤 설명에서는 bagging과 boosting모두*<br>
*weak learner들을 결합해 storng learner을*<br>
*생성하는 것이라고 나타내지만,*<br>
*특히 bagging에 대한 다른 설명에서는*<br>
*weak learner대신 base learner라는 표현을 사용합니다.*

*개인적으로는, bagging에서는*<br> 
*각 모델의 오류(weakness=residual)를 고려하지 않으므로*<br>
*base learner가 적합해보입니다.*


Resampling시, 
무작위로 샘플을 구성하며,<br>
각 모델들의 예측값이 서로 영향을 주지 않아<br>
서로 독립적입니다.(=병렬Parallel 앙상블)


##### Aggregating 집계

집계는, 분석의 목적이<br>
- Classification(분류)일 경우 과반수투표(Vote),<br>

- Regression(회귀)일 경우 평균치(Average)를 사용합니다.

대표적 예로는<br> 
traing과정에서 생성되는<br> 
여러 Decision Tree(의사결정나무=base learner)들을 
평균 혹은 과반수투표를 하나의 숲(강한 분류기)를 생성하는<br>
Random Forest(랜덤포레스트)가 있습니다.

-------------

#### 2. Boosting


boosting은 bagging의 변형으로<br>
각 모델에 데이터를 샘플링할 때, <br>
전 모델의 오류(=잔차 residual)에 
가중치를 부여하는 방법으로 오류를 해결하고자 합니다. 

(학원이나 학교로 따지면, 열등생 집중보충교육과 비슷,,)

*참고: 잔차란, 예측 후 남은 값, 즉 정답과 다른 값(=오류)을 말합니다.*

그렇기에 bagging과 달리<br>
boosting은 High Bias의 경우, 그 Bias를 낮추는 것에 적합합니다. 

또한, 무작위로 샘플을 구성하지만,<br> 
각 모델의 예측값이 서로 영향을 주므로<br>
순차적(Sequential)앙상블 기법이라고 할 수 있습니다.<br>

(= 잔차를 순차적으로 해결하므로 Residual Fitting의 하나라고도 볼 수 있음)


그리고, Boosting은 가중치 부여 방식에 따라<br>
여러 기법으로 나뉘는데 이들이 바로<br>

Kaggle 상위 랭킹 커널에서 사용되는
**Adaboost, GBM, LightGBM, Catboost 등을 말합니다.** 

여기선 Adaboost와 GBM을 우선적으로 이해해보려고 합니다.


#### 2.1 AdaBoost(아다부스트)

Adaboost는 Adaptive Boosting의 약자로,<br>
여기서 Adaptive은,<br>

전 모델의 오류에 가중치를 부여해<br>
다음 모델에서 오류를 해결하는 방식이<br>
모든 모델에 적용될 때까지 반복하여<br>
Weak learner들이 오류 해결 알고리즘에 **"적응"** 되는 것을 의미합니다.


위의 Boosting 설명에서 크게 벗어나지 않는,<br>
가장 기본적 알고리즘이라고 할 수 있습니다.

#### 2.2 GBM(Gradient Boosting Algorithms)

그렇다면 Gradient Boosting은 무엇일까요?

이를 이해하기 위해 우선<br>
Gradient Descent(경사 하강법)을 이해할 필요가 있습니다.

Gradient Descent은,<br>
함수의 기울기를 낮은 값으로 점점 이동시켜,<br>
극 값에 이르도록 하는 방법입니다.

손실 함수에서 손실 값을 줄이는 것에 사용되는 대표적 기법으로,<br> 
이 때 손실은, <br>
예측값 중 정답과 다른 값들을 의미한다고 할 수 있습니다. 


손실 함수에서 기울기는 벡터, 즉 크기와 방향을 지니며<br>
손실을 최소화하는 지점(즉, 극 값)으로 향하기 위해

시작점의 기울기에 반대방향<br>
(음의 기울기=Negative Gradient)을 취하고,

크기를 가늠해 다음 지점으로 향하는 방법을 반복하여<br>
결국 손실함수의 극 값으로 향하게 합니다.  

*참고: <br>
손실함수의 모든 기울기는 손실이 최대화되는 방향으로 향하므로,*<br>
*그 반대 방향을 취해주어야 합니다.*


GBM도 Adaboost와 비슷하지만

전 모델의 오류에 가중치를 부여하는 과정에서<br>
Gradient Descent를 통해<br>
최적의 파라미터를 찾는다는 점에서 차이점이 발생합니다.<br>

(그리고 이는 Total error의 최소점을 찾는 것과도 이어집니다.)


-------------

이렇게 머신러닝에서 발생하는<br>
학습 모델의 오류 2가지; Bias와 Variance,

그리고 이를 해결하기 위한 Ensemble기법과<br>
그 대표적인 방법 2가지; Bagging과 Boosting 을 이해할 수 있었습니다.

*위 내용은 본인의 티스토리 포스팅과 일치합니다. [tistory posting](https://todo-data.tistory.com/6)*


###### [reference1](https://www.slideshare.net/freepsw/boosting-bagging-vs-boosting)


###### [reference2](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent?hl=ko)


###### [reference3](https://becominghuman.ai/ensemble-learning-bagging-and-boosting-d20f38be9b1e)
